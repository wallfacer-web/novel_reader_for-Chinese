#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‹±æ–‡å°è¯´é˜…è¯»è¾…åŠ©è½¯ä»¶ (English Novel Reading Assistant)
ä¸€ä¸ªåŸºäºAIçš„è‹±æ–‡å°è¯´é˜…è¯»è¾…åŠ©å·¥å…·ï¼Œå¸®åŠ©ä¸­æ–‡ç”¨æˆ·æ›´å¥½åœ°ç†è§£å’Œå­¦ä¹ è‹±æ–‡å°è¯´ã€‚

Author: Toby LUO@ZHKU (903098625@qq.com)
Copyright (c) 2024 Toby LUO@ZHKU (903098625@qq.com)
License: MIT License

GitHub: https://github.com/wallfacer-web/novel_reader_for-Chinese
"""

import gradio as gr
import requests
import json
import re
from typing import List, Dict, Any, Tuple
import time
from docx import Document
from docx.shared import Inches
from docx.enum.text import WD_ALIGN_PARAGRAPH
import os
import logging
import math
from collections import Counter
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
import sqlite3
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

class VocabularyDatabase:
    """è¯æ±‡æ•°æ®åº“ç®¡ç†"""
    
    def __init__(self, db_path: str = "vocabulary.db"):
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºè¯æ±‡è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS vocabulary (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                word TEXT UNIQUE,
                definition TEXT,
                word_family TEXT,
                frequency_level INTEGER,
                learned_count INTEGER DEFAULT 0,
                first_seen DATE,
                last_reviewed DATE
            )
        ''')
        
        # åˆ›å»ºå­¦ä¹ è®°å½•è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS learning_progress (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_date DATE,
                words_learned INTEGER,
                paragraphs_processed INTEGER,
                reading_time INTEGER
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def add_word(self, word: str, definition: str, word_family: str = "", frequency_level: int = 5):
        """æ·»åŠ å•è¯åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
                INSERT OR REPLACE INTO vocabulary 
                (word, definition, word_family, frequency_level, first_seen)
                VALUES (?, ?, ?, ?, ?)
            ''', (word.lower(), definition, word_family, frequency_level, datetime.now().date()))
            conn.commit()
        except Exception as e:
            logger.error(f"æ·»åŠ å•è¯å¤±è´¥: {e}")
        finally:
            conn.close()
    
    def get_learned_words(self) -> List[str]:
        """è·å–å·²å­¦ä¹ çš„å•è¯åˆ—è¡¨"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT word FROM vocabulary WHERE learned_count > 0')
        words = [row[0] for row in cursor.fetchall()]
        
        conn.close()
        return words

class TextDifficultyAnalyzer:
    """æ–‡æœ¬éš¾åº¦åˆ†æå™¨"""
    
    def __init__(self):
        # åŸºç¡€å¸¸ç”¨è¯æ±‡è¡¨ï¼ˆæ¨¡æ‹Ÿå‰3000ä¸ªæœ€å¸¸ç”¨è‹±è¯­å•è¯ï¼‰
        self.common_words = self._load_basic_words()
    
    def _load_basic_words(self) -> set:
        """åŠ è½½åŸºç¡€è¯æ±‡è¡¨"""
        basic_words = [
            'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have',
            'i', 'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you',
            'do', 'at', 'this', 'but', 'his', 'by', 'from', 'they',
            'she', 'or', 'an', 'will', 'my', 'one', 'all', 'would',
            'there', 'their', 'what', 'so', 'up', 'out', 'if', 'about',
            'who', 'get', 'which', 'go', 'me', 'when', 'make', 'can',
            'like', 'time', 'no', 'just', 'him', 'know', 'take', 'people',
            'into', 'year', 'your', 'good', 'some', 'could', 'them', 'see',
            'other', 'than', 'then', 'now', 'look', 'only', 'come', 'its',
            'over', 'think', 'also', 'back', 'after', 'use', 'two', 'how',
            'our', 'work', 'first', 'well', 'way', 'even', 'new', 'want',
            'because', 'any', 'these', 'give', 'day', 'most', 'us', 'was',
            'been', 'said', 'each', 'which', 'she', 'do', 'how', 'their',
            'if', 'will', 'up', 'other', 'about', 'out', 'many', 'then',
            'them', 'these', 'so', 'some', 'her', 'would', 'make', 'like',
            'into', 'him', 'has', 'more', 'go', 'no', 'way', 'could', 'my',
            'than', 'first', 'water', 'been', 'call', 'who', 'its', 'now',
            'find', 'long', 'down', 'day', 'did', 'get', 'come', 'made',
            'may', 'part', 'over', 'new', 'sound', 'take', 'only', 'little',
            'work', 'know', 'place', 'year', 'live', 'me', 'back', 'give',
            'most', 'very', 'after', 'thing', 'our', 'name', 'good', 'sentence',
            'man', 'think', 'say', 'great', 'where', 'help', 'through', 'much',
            'before', 'line', 'right', 'too', 'mean', 'old', 'any', 'same',
            'tell', 'boy', 'follow', 'came', 'want', 'show', 'also', 'around',
            'form', 'three', 'small', 'set', 'put', 'end', 'why', 'again',
            'turn', 'here', 'off', 'went', 'old', 'number', 'great', 'tell',
            'men', 'say', 'small', 'every', 'found', 'still', 'between',
            'mane', 'should', 'home', 'big', 'give', 'air', 'line', 'set',
            'own', 'under', 'read', 'last', 'never', 'us', 'left', 'end',
            'along', 'while', 'might', 'next', 'sound', 'below', 'saw',
            'something', 'thought', 'both', 'few', 'those', 'always', 'looked',
            'show', 'large', 'often', 'together', 'asked', 'house', 'world',
            'going', 'want', 'school', 'important', 'until', 'without', 'form',
            'black', 'white', 'words', 'students', 'during', 'started', 'include',
            'young', 'book', 'example', 'took', 'being', 'different', 'state',
            'never', 'became', 'between', 'high', 'really', 'something', 'most',
            'another', 'much', 'family', 'own', 'out', 'leave', 'put', 'old',
            'while', 'mean', 'on', 'keep', 'student', 'why', 'let', 'great',
            'same', 'big', 'group', 'begin', 'seem', 'country', 'help', 'talk',
            'where', 'turn', 'problem', 'every', 'start', 'hand', 'might',
            'american', 'show', 'part', 'about', 'against', 'place', 'over',
            'such', 'again', 'few', 'case', 'most', 'week', 'company', 'where',
            'system', 'each', 'right', 'program', 'hear', 'so', 'question',
            'during', 'work', 'play', 'government', 'run', 'small', 'number',
            'off', 'always', 'move', 'like', 'night', 'live', 'mr', 'point',
            'believe', 'hold', 'today', 'bring', 'happen', 'next', 'without',
            'before', 'large', 'all', 'million', 'must', 'home', 'under', 'water',
            'room', 'write', 'mother', 'area', 'national', 'money', 'story',
            'young', 'fact', 'month', 'different', 'lot', 'right', 'study',
            'book', 'eye', 'job', 'word', 'though', 'business', 'issue', 'side',
            'kind', 'four', 'head', 'far', 'black', 'long', 'both', 'little',
            'house', 'yes', 'after', 'since', 'long', 'provide', 'service',
            'around', 'friend', 'important', 'father', 'sit', 'away', 'until',
            'power', 'hour', 'game', 'often', 'yet', 'line', 'political', 'end',
            'among', 'ever', 'stand', 'bad', 'lose', 'however', 'member', 'pay',
            'law', 'meet', 'car', 'city', 'almost', 'include', 'continue',
            'set', 'later', 'community', 'much', 'name', 'five', 'once', 'white',
            'least', 'president', 'learn', 'real', 'change', 'team', 'minute',
            'best', 'several', 'idea', 'kid', 'body', 'information', 'back',
            'parent', 'face', 'others', 'level', 'office', 'door', 'health',
            'person', 'art', 'war', 'history', 'party', 'within', 'grow',
            'result', 'open', 'change', 'morning', 'walk', 'reason', 'low',
            'win', 'research', 'girl', 'guy', 'early', 'food', 'before', 'moment',
            'himself', 'air', 'teacher', 'force', 'offer'
        ]
        return set(basic_words)
    
    def analyze_text_difficulty(self, text: str) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬éš¾åº¦"""
        # ç®€å•çš„è¯æ±‡åˆ†æ
        words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
        total_words = len(words)
        unique_words = len(set(words))
        
        # å¥å­åˆ†æ
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # è®¡ç®—å¸¸ç”¨è¯æ¯”ä¾‹
        common_word_count = sum(1 for word in words if word in self.common_words)
        common_word_ratio = common_word_count / total_words if total_words > 0 else 0
        
        # è®¡ç®—å¹³å‡å¥é•¿
        avg_sentence_length = total_words / len(sentences) if sentences else 0
        
        # è¯†åˆ«éš¾è¯
        difficult_words = [word for word in set(words) 
                         if word not in self.common_words and len(word) > 3]
        
        # è®¡ç®—éš¾åº¦è¯„åˆ† (1-10, 10æœ€éš¾)
        difficulty_score = self._calculate_difficulty_score(
            common_word_ratio, avg_sentence_length, len(difficult_words), unique_words
        )
        
        return {
            'total_words': total_words,
            'unique_words': unique_words,
            'common_word_ratio': common_word_ratio,
            'avg_sentence_length': avg_sentence_length,
            'difficult_words': difficult_words[:15],
            'difficulty_score': difficulty_score,
            'reading_level': self._get_reading_level(difficulty_score),
            'estimated_reading_time': self._estimate_reading_time(total_words),
            'vocabulary_coverage': common_word_ratio * 100
        }
    
    def _calculate_difficulty_score(self, common_ratio: float, avg_sent_len: float, 
                                  difficult_count: int, unique_count: int) -> float:
        """è®¡ç®—éš¾åº¦è¯„åˆ†"""
        # åŸºäºç ”ç©¶æ–‡çŒ®çš„98%è¯æ±‡è¦†ç›–ç‡åŸåˆ™
        coverage_penalty = max(0, (0.98 - common_ratio) * 8)
        sentence_penalty = max(0, (avg_sent_len - 15) * 0.15)
        difficulty_penalty = min(difficult_count * 0.08, 2.5)
        
        base_score = 5
        total_penalty = coverage_penalty + sentence_penalty + difficulty_penalty
        
        return min(10, max(1, base_score + total_penalty))
    
    def _get_reading_level(self, score: float) -> str:
        """æ ¹æ®è¯„åˆ†è·å–é˜…è¯»æ°´å¹³"""
        if score <= 3:
            return "åˆçº§ (é€‚åˆåˆå­¦è€…)"
        elif score <= 5:
            return "åˆä¸­çº§ (é€‚åˆæœ‰åŸºç¡€çš„å­¦ä¹ è€…)"
        elif score <= 7:
            return "ä¸­çº§ (é€‚åˆä¸­ç­‰æ°´å¹³å­¦ä¹ è€…)"
        elif score <= 8.5:
            return "ä¸­é«˜çº§ (éœ€è¦è¾ƒå¥½çš„è‹±è¯­åŸºç¡€)"
        else:
            return "é«˜çº§ (éœ€è¦æ‰å®çš„è‹±è¯­åŠŸåº•)"
    
    def _estimate_reading_time(self, word_count: int) -> str:
        """ä¼°ç®—é˜…è¯»æ—¶é—´"""
        minutes = word_count / 130  # ä¸­å›½EFLå­¦ä¹ è€…å¹³å‡é˜…è¯»é€Ÿåº¦
        
        if minutes < 1:
            return f"{int(minutes * 60)}ç§’"
        elif minutes < 60:
            return f"{int(minutes)}åˆ†é’Ÿ"
        else:
            hours = int(minutes / 60)
            remaining_minutes = int(minutes % 60)
            return f"{hours}å°æ—¶{remaining_minutes}åˆ†é’Ÿ"

class EnhancedNovelReader:
    """å¢å¼ºç‰ˆå°è¯´é˜…è¯»åŠ©æ‰‹"""
    
    def __init__(self, model_name: str = "huihui_ai/qwenlong-abliterated:latest"):
        self.model_name = model_name
        self.ollama_url = "http://localhost:11434/api/generate"
        self.processed_paragraphs = []
        self.difficulty_analyzer = TextDifficultyAnalyzer()
        self.vocab_db = VocabularyDatabase()
        # å¯ç”¨æ¨¡å‹åˆ—è¡¨
        self.available_models = [
            "huihui_ai/qwenlong-abliterated:latest",
            "gemma3:12b",
            "gemma3:27b", 
            "qwen3:32b",
            "qwen3:8b",
            "gemma3:4b",
            "phi4:latest"
        ]
    
    def set_model(self, model_name: str):
        """è®¾ç½®ä½¿ç”¨çš„æ¨¡å‹"""
        if model_name in self.available_models:
            self.model_name = model_name
            logger.info(f"æ¨¡å‹å·²åˆ‡æ¢ä¸º: {model_name}")
        else:
            logger.warning(f"æ¨¡å‹ {model_name} ä¸åœ¨å¯ç”¨åˆ—è¡¨ä¸­")
    
    def create_enhanced_analysis_prompt(self, paragraph: str, difficulty_info: Dict) -> str:
        """åˆ›å»ºå¢å¼ºçš„åˆ†ææç¤ºè¯ï¼ˆç”¨äºå•æ®µè½è¯¦ç»†åˆ†æï¼Œä¿ç•™æ€ç»´é“¾ï¼‰"""
        prompt = f"""
ä½œä¸ºè‹±è¯­æ•™å­¦ä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹è‹±æ–‡å°è¯´æ®µè½è¿›è¡Œæ·±åº¦åˆ†æï¼Œç‰¹åˆ«å…³æ³¨ä¸­å›½å­¦ç”Ÿçš„å­¦ä¹ éœ€æ±‚ï¼š

ã€åŸæ–‡æ®µè½ã€‘
{paragraph}

ã€æ®µè½åŸºæœ¬ä¿¡æ¯ã€‘
- æ€»è¯æ•°ï¼š{difficulty_info['total_words']}
- ç‹¬ç‰¹è¯æ±‡ï¼š{difficulty_info['unique_words']}
- è¯æ±‡è¦†ç›–ç‡ï¼š{difficulty_info['vocabulary_coverage']:.1f}%
- éš¾åº¦ç­‰çº§ï¼š{difficulty_info['reading_level']}
- é¢„ä¼°é˜…è¯»æ—¶é—´ï¼š{difficulty_info['estimated_reading_time']}

è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„è¿›è¡Œè¯¦ç»†åˆ†æï¼š

## ğŸ“Š æ–‡æœ¬éš¾åº¦è¯„ä¼°
- æ ¹æ®98%è¯æ±‡è¦†ç›–ç‡åŸåˆ™ï¼Œè¯„ä¼°æ­¤æ®µè½å¯¹ä¸­å›½å­¦ç”Ÿçš„éš¾åº¦
- æŒ‡å‡ºå¯èƒ½é€ æˆç†è§£éšœç¢çš„è¯­è¨€ç‰¹å¾

## ğŸ“š æ ¸å¿ƒè¯æ±‡æ·±åº¦è§£æ
è¯·é€‰æ‹©5-8ä¸ªå…³é”®è¯æ±‡è¿›è¡Œæ·±åº¦åˆ†æï¼ŒåŒ…æ‹¬ï¼š
- è¯æ±‡çš„åŸºæœ¬å«ä¹‰å’Œè¯æ€§
- è¯æ—å…³ç³»ï¼ˆå¦‚ï¼šinform â†’ information, informative, informantï¼‰
- å¸¸ç”¨æ­é…å’Œå›ºå®šç”¨æ³•
- åœ¨ä¸åŒè¯­å¢ƒä¸­çš„å«ä¹‰å˜åŒ–
- åŒä¹‰è¯å’Œåä¹‰è¯

## ğŸ›ï¸ è¯­è¨€ç»“æ„åˆ†æ
- è¯†åˆ«å¤æ‚å¥å¼ç»“æ„ï¼ˆå¦‚å€’è£…ã€çœç•¥ã€åµŒå¥—ç­‰ï¼‰
- è§£é‡Šå¯èƒ½å›°æ‰°ä¸­å›½å­¦ç”Ÿçš„è¯­æ³•ç°è±¡
- å¯¹æ¯”ä¸­è‹±è¯­è¨€å·®å¼‚

## ğŸŒ æ–‡åŒ–èƒŒæ™¯æ·±åº¦è§£è¯»
- è¯¦ç»†è§£é‡Šè¥¿æ–¹æ–‡åŒ–èƒŒæ™¯çŸ¥è¯†
- å†å²ã€ç¤¾ä¼šã€å®—æ•™èƒŒæ™¯è¯´æ˜
- å¸®åŠ©ç†è§£æ–‡æœ¬ä¸­çš„æ–‡åŒ–å†…æ¶µ

## ğŸ­ æ–‡å­¦æŠ€å·§ä¸å°è¯´è¦ç´ 
- å™äº‹è§†è§’å’ŒæŠ€å·§åˆ†æ
- äººç‰©å¡‘é€ å’Œæ€§æ ¼æå†™
- æƒ…èŠ‚å‘å±•å’Œæ–‡å­¦æ‰‹æ³•
- è±¡å¾æ„ä¹‰å’Œéšå–»è§£è¯»

## ğŸ¯ é˜…è¯»ç­–ç•¥æŒ‡å¯¼
åŸºäºç ”ç©¶å»ºè®®ï¼Œæä¾›å…·ä½“çš„é˜…è¯»ç­–ç•¥ï¼š
- é¢„æµ‹å’Œæ¨ç†æŠ€å·§
- ä¸Šä¸‹æ–‡æ¨æ–­æ–¹æ³•
- å¦‚ä½•å¤„ç†ç”Ÿè¯
- æé«˜é˜…è¯»æµç•…åº¦çš„å»ºè®®

## ğŸ’¡ æ€è€ƒé—®é¢˜
è®¾è®¡3-5ä¸ªæ·±å±‚æ€è€ƒé—®é¢˜ï¼Œä¿ƒè¿›ï¼š
- æ‰¹åˆ¤æ€§æ€ç»´
- æ–‡æœ¬ç†è§£
- è·¨æ–‡åŒ–æ¯”è¾ƒ
- ä¸ªäººåæ€

## ğŸ§  è®°å¿†ä¸ç†è§£æ£€æŸ¥
- æ®µè½æ ¸å¿ƒä¿¡æ¯æ¦‚æ‹¬
- å…³é”®ç»†èŠ‚å›é¡¾
- ç†è§£ç¨‹åº¦è‡ªæµ‹é—®é¢˜

## ğŸˆ¶ ç²¾å‡†ä¸­æ–‡ç¿»è¯‘
æä¾›ä¸¤ä¸ªç‰ˆæœ¬çš„ç¿»è¯‘ï¼š
1. ç›´è¯‘ç‰ˆæœ¬ï¼ˆä¿æŒåŸæ–‡ç»“æ„ï¼‰
2. æ„è¯‘ç‰ˆæœ¬ï¼ˆç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯ï¼‰

è¯·ç¡®ä¿åˆ†æè¯¦ç»†ã€å‡†ç¡®ï¼Œç‰¹åˆ«å…³æ³¨ä¸­å›½å­¦ç”Ÿçš„å­¦ä¹ ç‰¹ç‚¹å’Œéœ€æ±‚ã€‚
"""
        return prompt
    
    def create_simplified_analysis_prompt(self, paragraph: str, difficulty_info: Dict) -> str:
        """åˆ›å»ºç®€åŒ–çš„åˆ†ææç¤ºè¯ï¼ˆç”¨äºæ•´æœ¬ä¹¦å¤„ç†ï¼Œå…³é—­æ€ç»´é“¾ä»¥æé«˜é€Ÿåº¦ï¼‰"""
        prompt = f"""
è¯·å¯¹ä»¥ä¸‹è‹±æ–‡å°è¯´æ®µè½è¿›è¡Œå¿«é€Ÿåˆ†æï¼Œä¸ºä¸­å›½å­¦ç”Ÿæä¾›å…³é”®ä¿¡æ¯ï¼š

ã€åŸæ–‡æ®µè½ã€‘
{paragraph}

ã€æ®µè½ä¿¡æ¯ã€‘è¯æ•°ï¼š{difficulty_info['total_words']}ï¼Œéš¾åº¦ï¼š{difficulty_info['reading_level']}

è¯·æä¾›ç®€æ´åˆ†æï¼š

## ğŸ“š å…³é”®è¯æ±‡ï¼ˆ3-5ä¸ªï¼‰
é€‰æ‹©æœ€é‡è¦çš„è¯æ±‡ï¼Œç®€è¦è¯´æ˜å«ä¹‰å’Œç”¨æ³•ã€‚

## ğŸŒ æ–‡åŒ–èƒŒæ™¯
ç®€è¦è¯´æ˜é‡è¦çš„æ–‡åŒ–èƒŒæ™¯çŸ¥è¯†ã€‚

## ğŸ­ æ–‡å­¦è¦ç´ 
ç®€è¦åˆ†æå™äº‹æŠ€å·§å’Œæ–‡å­¦æ‰‹æ³•ã€‚

## ğŸˆ¶ ä¸­æ–‡ç¿»è¯‘
æä¾›å‡†ç¡®çš„ä¸­æ–‡ç¿»è¯‘ã€‚

è¯·ä¿æŒç®€æ´ï¼Œé‡ç‚¹çªå‡ºæ ¸å¿ƒå†…å®¹ã€‚
"""
        return prompt
    
    def call_ollama(self, prompt: str, is_simplified: bool = False) -> str:
        """è°ƒç”¨ollamaæ¨¡å‹
        
        Args:
            prompt: æç¤ºè¯
            is_simplified: æ˜¯å¦ä¸ºç®€åŒ–åˆ†æï¼ˆç”¨äºä¼˜åŒ–å‚æ•°ï¼‰
        """
        try:
            # æ ¹æ®åˆ†æç±»å‹è°ƒæ•´å‚æ•°
            if is_simplified:
                # ç®€åŒ–åˆ†æï¼šæ›´ä½çš„æ¸©åº¦ï¼Œæ›´å°‘çš„tokensï¼Œæ›´çŸ­çš„è¶…æ—¶
                options = {
                    "temperature": 0.1,
                    "top_p": 0.8,
                    "max_tokens": 2000,
                    "repeat_penalty": 1.0,
                }
                timeout = 120  # æ›´çŸ­çš„è¶…æ—¶æ—¶é—´
            else:
                # è¯¦ç»†åˆ†æï¼šæ ‡å‡†å‚æ•°
                options = {
                    "temperature": 0.3,
                    "top_p": 0.9,
                    "max_tokens": 6000,
                    "repeat_penalty": 1.1,
                }
                timeout = 300
            
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": options
            }
            
            response = requests.post(self.ollama_url, json=payload, timeout=timeout)
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '')
            else:
                logger.error(f"Ollama API error: {response.status_code}")
                return f"é”™è¯¯ï¼šAPIè°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}"
                
        except Exception as e:
            logger.error(f"Error calling Ollama: {str(e)}")
            return f"é”™è¯¯ï¼š{str(e)}"
    
    def analyze_paragraph(self, paragraph: str, index: int, use_detailed_analysis: bool = True) -> Dict[str, Any]:
        """åˆ†ææ®µè½
        
        Args:
            paragraph: è¦åˆ†æçš„æ®µè½æ–‡æœ¬
            index: æ®µè½ç´¢å¼•
            use_detailed_analysis: æ˜¯å¦ä½¿ç”¨è¯¦ç»†åˆ†æï¼ˆTrue=è¯¦ç»†åˆ†æï¼ŒFalse=ç®€åŒ–åˆ†æï¼‰
        """
        analysis_type = "è¯¦ç»†" if use_detailed_analysis else "ç®€åŒ–"
        logger.info(f"æ­£åœ¨è¿›è¡Œ{analysis_type}åˆ†æç¬¬ {index + 1} æ®µè½...")
        
        # è¿›è¡Œéš¾åº¦åˆ†æ
        difficulty_info = self.difficulty_analyzer.analyze_text_difficulty(paragraph)
        
        # æ ¹æ®åˆ†æç±»å‹é€‰æ‹©æç¤ºè¯
        if use_detailed_analysis:
            prompt = self.create_enhanced_analysis_prompt(paragraph, difficulty_info)
        else:
            prompt = self.create_simplified_analysis_prompt(paragraph, difficulty_info)
        
        # è·å–AIåˆ†æ
        analysis = self.call_ollama(prompt, is_simplified=not use_detailed_analysis)
        
        # æå–å¹¶ä¿å­˜è¯æ±‡
        self._extract_and_save_vocabulary(paragraph, analysis)
        
        result = {
            "index": index + 1,
            "original_text": paragraph,
            "difficulty_info": difficulty_info,
            "analysis": analysis,
            "analysis_type": analysis_type,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        self.processed_paragraphs.append(result)
        return result
    
    def _extract_and_save_vocabulary(self, text: str, analysis: str):
        """ä»æ–‡æœ¬å’Œåˆ†æä¸­æå–è¯æ±‡å¹¶ä¿å­˜åˆ°æ•°æ®åº“"""
        # ç®€å•çš„è¯æ±‡æå–ï¼ˆå¯ä»¥åç»­æ”¹è¿›ï¼‰
        words = word_tokenize(text.lower())
        words = [word for word in words if word.isalpha() and len(word) > 3]
        
        for word in set(words):
            if word not in self.difficulty_analyzer.common_words:
                # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„è¯æ±‡å®šä¹‰æå–é€»è¾‘
                self.vocab_db.add_word(word, "", "", 5)
    
    def get_reading_recommendations(self, difficulty_score: float) -> List[str]:
        """æ ¹æ®éš¾åº¦è¯„åˆ†æä¾›é˜…è¯»å»ºè®®"""
        recommendations = []
        
        if difficulty_score > 8:
            recommendations.extend([
                "ğŸš¨ æ­¤æ–‡æœ¬éš¾åº¦è¾ƒé«˜ï¼Œå»ºè®®ï¼š",
                "â€¢ å…ˆé¢„ä¹ å…³é”®è¯æ±‡å’Œæ–‡åŒ–èƒŒæ™¯",
                "â€¢ åˆ†æ®µé˜…è¯»ï¼Œä¸è¦æ€¥äºæ±‚æˆ",
                "â€¢ ä½¿ç”¨è¯å…¸å’Œåœ¨çº¿èµ„æºè¾…åŠ©ç†è§£",
                "â€¢ è€ƒè™‘å…ˆé˜…è¯»ç®€åŒ–ç‰ˆæœ¬"
            ])
        elif difficulty_score > 6:
            recommendations.extend([
                "âš ï¸ æ­¤æ–‡æœ¬å…·æœ‰ä¸€å®šæŒ‘æˆ˜æ€§ï¼Œå»ºè®®ï¼š",
                "â€¢ åœ¨é˜…è¯»å‰æµè§ˆä¸€éï¼Œäº†è§£å¤§æ„",
                "â€¢ é‡ç‚¹å…³æ³¨æ®µè½ä¸»é¢˜å¥",
                "â€¢ é‡åˆ°ç”Ÿè¯å…ˆå°è¯•çŒœæµ‹å«ä¹‰",
                "â€¢ é€‚å½“ä½¿ç”¨è¯å…¸è¾…åŠ©"
            ])
        else:
            recommendations.extend([
                "âœ… æ­¤æ–‡æœ¬éš¾åº¦é€‚ä¸­ï¼Œå»ºè®®ï¼š",
                "â€¢ ä¿æŒæµç•…é˜…è¯»ï¼Œä¸è¦è¿‡åˆ†çº ç»“ç”Ÿè¯",
                "â€¢ æ³¨æ„æ–‡ç« çš„é€»è¾‘ç»“æ„",
                "â€¢ å°è¯•é¢„æµ‹åç»­æƒ…èŠ‚å‘å±•",
                "â€¢ äº«å—é˜…è¯»è¿‡ç¨‹"
            ])
        
        return recommendations
    
    def split_text_into_paragraphs(self, text: str) -> List[str]:
        """æ™ºèƒ½åˆ†å‰²æ–‡æœ¬ä¸ºæ®µè½"""
        paragraphs = re.split(r'\n\s*\n', text.strip())
        
        cleaned_paragraphs = []
        for p in paragraphs:
            p = p.strip()
            if p and len(p.split()) >= 25:  # è‡³å°‘25ä¸ªè¯
                cleaned_paragraphs.append(p)
        
        return cleaned_paragraphs
    
    def create_enhanced_docx(self, novel_title: str = "è‹±æ–‡å°è¯´é˜…è¯»åˆ†ææŠ¥å‘Š") -> str:
        """åˆ›å»ºå¢å¼ºç‰ˆDOCXæ–‡æ¡£"""
        doc = Document()
        
        title = doc.add_heading('ğŸ“š ' + novel_title, 0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        
        doc.add_paragraph(f"ç”Ÿæˆæ—¶é—´ï¼š{time.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
        doc.add_paragraph(f"å¤„ç†æ®µè½æ•°ï¼š{len(self.processed_paragraphs)}")
        doc.add_paragraph("åŸºäºã€ŠHow to read English novels for Chinese studentsã€‹ç ”ç©¶æ–‡çŒ®")
        
        # æ·»åŠ åˆ†ææ¨¡å¼ä¿¡æ¯
        if self.processed_paragraphs:
            analysis_mode = self.processed_paragraphs[0].get('analysis_type', 'è¯¦ç»†')
            doc.add_paragraph(f"åˆ†ææ¨¡å¼ï¼š{analysis_mode}åˆ†æ")
            if analysis_mode == "ç®€åŒ–":
                doc.add_paragraph("âš¡ é‡‡ç”¨å¿«é€Ÿåˆ†ææ¨¡å¼ï¼Œå…³é—­æ€ç»´é“¾ä»¥æé«˜å¤„ç†é€Ÿåº¦")
        
        doc.add_paragraph("=" * 60)
        
        if self.processed_paragraphs:
            # æ·»åŠ æ€»ä½“ç»Ÿè®¡
            doc.add_heading('ğŸ“Š é˜…è¯»ç»Ÿè®¡æ¦‚è§ˆ', level=1)
            
            total_words = sum(p['difficulty_info']['total_words'] for p in self.processed_paragraphs)
            avg_difficulty = sum(p['difficulty_info']['difficulty_score'] for p in self.processed_paragraphs) / len(self.processed_paragraphs)
            
            doc.add_paragraph(f"â€¢ æ€»è¯æ•°ï¼š{total_words}")
            doc.add_paragraph(f"â€¢ å¹³å‡éš¾åº¦è¯„åˆ†ï¼š{avg_difficulty:.1f}/10")
            doc.add_paragraph(f"â€¢ é¢„ä¼°æ€»é˜…è¯»æ—¶é—´ï¼š{self.difficulty_analyzer._estimate_reading_time(total_words)}")
            
            # æ·»åŠ æ¯ä¸ªæ®µè½çš„åˆ†æ
            for paragraph_data in self.processed_paragraphs:
                doc.add_page_break()
                
                doc.add_heading(f"ç¬¬ {paragraph_data['index']} æ®µ", level=1)
                
                difficulty_info = paragraph_data['difficulty_info']
                doc.add_heading('ğŸ“Š éš¾åº¦è¯„ä¼°', level=2)
                doc.add_paragraph(f"éš¾åº¦è¯„åˆ†ï¼š{difficulty_info['difficulty_score']:.1f}/10")
                doc.add_paragraph(f"é˜…è¯»ç­‰çº§ï¼š{difficulty_info['reading_level']}")
                doc.add_paragraph(f"è¯æ±‡è¦†ç›–ç‡ï¼š{difficulty_info['vocabulary_coverage']:.1f}%")
                
                doc.add_heading('ğŸ“– åŸæ–‡', level=2)
                p = doc.add_paragraph(paragraph_data['original_text'])
                p.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
                
                doc.add_heading('ğŸ” è¯¦ç»†åˆ†æ', level=2)
                analysis_p = doc.add_paragraph(paragraph_data['analysis'])
                analysis_p.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
                
                recommendations = self.get_reading_recommendations(difficulty_info['difficulty_score'])
                if recommendations:
                    doc.add_heading('ğŸ’¡ é˜…è¯»å»ºè®®', level=2)
                    for rec in recommendations:
                        doc.add_paragraph(rec)
        
        filename = f"enhanced_novel_analysis_{time.strftime('%Y%m%d_%H%M%S')}.docx"
        doc.save(filename)
        return filename

class EnhancedGradioInterface:
    """å¢å¼ºç‰ˆGradioç•Œé¢"""
    
    def __init__(self):
        self.reader = EnhancedNovelReader()
        self.current_paragraphs = []
        self.current_index = 0
        self.current_novel_title = "æœªå‘½åå°è¯´"
        self.current_model = self.reader.model_name
    
    def change_model(self, model_name: str) -> str:
        """åˆ‡æ¢æ¨¡å‹"""
        self.reader.set_model(model_name)
        self.current_model = model_name
        return f"âœ… å·²åˆ‡æ¢åˆ°æ¨¡å‹ï¼š{model_name}"
        
    def handle_file_upload(self, uploaded_file_path) -> Tuple[str, str]:
        """å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶"""
        try:
            if uploaded_file_path is None or uploaded_file_path == "":
                return "âŒ è¯·é€‰æ‹©è¦ä¸Šä¼ çš„æ–‡ä»¶", ""
            
            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            if not uploaded_file_path.lower().endswith(('.txt', '.md')):
                return "âŒ åªæ”¯æŒ .txt å’Œ .md æ ¼å¼çš„æ–‡ä»¶", ""
            
            # è·å–æ–‡ä»¶å
            self.current_novel_title = os.path.splitext(os.path.basename(uploaded_file_path))[0]
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(uploaded_file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            return self._load_content(content)
            
        except Exception as e:
            return f"âŒ ä¸Šä¼ æ–‡ä»¶æ—¶å‡ºé”™ï¼š{str(e)}", ""
    
    def load_and_analyze_novel(self, file_path: str) -> Tuple[str, str]:
        """åŠ è½½å¹¶åˆ†æå°è¯´"""
        try:
            if file_path and os.path.exists(file_path):
                self.current_novel_title = os.path.splitext(os.path.basename(file_path))[0]
                
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                return self._load_content(content)
            else:
                return "âŒ æ–‡ä»¶è·¯å¾„æ— æ•ˆæˆ–æ–‡ä»¶ä¸å­˜åœ¨", ""
        except Exception as e:
            return f"âŒ åŠ è½½æ–‡ä»¶æ—¶å‡ºé”™ï¼š{str(e)}", ""
    
    def _load_content(self, content: str) -> Tuple[str, str]:
        """åŠ è½½å†…å®¹çš„å…±åŒé€»è¾‘"""
        self.current_paragraphs = self.reader.split_text_into_paragraphs(content)
        self.current_index = 0
        self.reader.processed_paragraphs = []
        
        overall_difficulty = self.reader.difficulty_analyzer.analyze_text_difficulty(content)
        
        status_message = f"âœ… æˆåŠŸåŠ è½½å°è¯´ã€Š{self.current_novel_title}ã€‹ï¼Œå…± {len(self.current_paragraphs)} æ®µè½"
        
        difficulty_summary = f"""ğŸ“Š æ•´ä½“éš¾åº¦åˆ†æï¼š
â€¢ æ€»è¯æ•°ï¼š{overall_difficulty['total_words']:,}
â€¢ ç‹¬ç‰¹è¯æ±‡ï¼š{overall_difficulty['unique_words']:,}
â€¢ è¯æ±‡è¦†ç›–ç‡ï¼š{overall_difficulty['vocabulary_coverage']:.1f}%
â€¢ éš¾åº¦è¯„åˆ†ï¼š{overall_difficulty['difficulty_score']:.1f}/10
â€¢ é˜…è¯»ç­‰çº§ï¼š{overall_difficulty['reading_level']}
â€¢ é¢„ä¼°é˜…è¯»æ—¶é—´ï¼š{overall_difficulty['estimated_reading_time']}

ğŸ’¡ é˜…è¯»å»ºè®®ï¼š
{chr(10).join(self.reader.get_reading_recommendations(overall_difficulty['difficulty_score']))}"""
        
        return status_message, difficulty_summary
    
    def process_next_paragraph(self) -> Tuple[str, str, str, str]:
        """å¤„ç†ä¸‹ä¸€ä¸ªæ®µè½"""
        if not self.current_paragraphs:
            return "âŒ è¯·å…ˆåŠ è½½å°è¯´æ–‡ä»¶", "", "", ""
        
        if self.current_index >= len(self.current_paragraphs):
            return "âœ… æ‰€æœ‰æ®µè½å·²å¤„ç†å®Œæˆ", "", "", ""
        
        current_paragraph = self.current_paragraphs[self.current_index]
        result = self.reader.analyze_paragraph(current_paragraph, self.current_index)
        self.current_index += 1
        
        progress_info = f"å·²å¤„ç† {self.current_index}/{len(self.current_paragraphs)} æ®µè½"
        
        difficulty_info = result['difficulty_info']
        difficulty_display = f"""ğŸ“Š å½“å‰æ®µè½éš¾åº¦ï¼š
â€¢ éš¾åº¦è¯„åˆ†ï¼š{difficulty_info['difficulty_score']:.1f}/10
â€¢ é˜…è¯»ç­‰çº§ï¼š{difficulty_info['reading_level']}
â€¢ è¯æ±‡è¦†ç›–ç‡ï¼š{difficulty_info['vocabulary_coverage']:.1f}%
â€¢ é¢„ä¼°é˜…è¯»æ—¶é—´ï¼š{difficulty_info['estimated_reading_time']}
â€¢ æ€»è¯æ•°ï¼š{difficulty_info['total_words']}ï¼Œç‹¬ç‰¹è¯æ±‡ï¼š{difficulty_info['unique_words']}"""
        
        return progress_info, difficulty_display, result['original_text'], result['analysis']
    
    def process_entire_novel(self) -> str:
        """å¤„ç†æ•´æœ¬å°è¯´ï¼ˆä½¿ç”¨ç®€åŒ–åˆ†ææ¨¡å¼ï¼Œå…³é—­æ€ç»´é“¾ä»¥æé«˜é€Ÿåº¦ï¼‰"""
        if not self.current_paragraphs:
            return "âŒ è¯·å…ˆåŠ è½½å°è¯´æ–‡ä»¶"
        
        try:
            total_paragraphs = len(self.current_paragraphs)
            logger.info(f"å¼€å§‹å¤„ç†æ•´æœ¬å°è¯´ã€Š{self.current_novel_title}ã€‹ï¼Œå…± {total_paragraphs} æ®µè½")
            logger.info("ğŸ“ˆ ä½¿ç”¨ç®€åŒ–åˆ†ææ¨¡å¼ï¼Œå…³é—­æ€ç»´é“¾ä»¥æé«˜å¤„ç†é€Ÿåº¦")
            
            # é‡ç½®å¤„ç†çŠ¶æ€
            self.reader.processed_paragraphs = []
            
            # å¤„ç†æ‰€æœ‰æ®µè½ - ä½¿ç”¨ç®€åŒ–åˆ†ææ¨¡å¼ï¼ˆå…³é—­æ€ç»´é“¾ï¼‰
            for i, paragraph in enumerate(self.current_paragraphs):
                logger.info(f"æ­£åœ¨å¿«é€Ÿå¤„ç†ç¬¬ {i+1}/{total_paragraphs} æ®µè½")
                # ä½¿ç”¨ use_detailed_analysis=False æ¥å…³é—­æ€ç»´é“¾
                result = self.reader.analyze_paragraph(paragraph, i, use_detailed_analysis=False)
            
            # ä¿å­˜å®Œæ•´åˆ†æ
            filename = self.reader.create_enhanced_docx(self.current_novel_title)
            
            final_message = f"""âœ… æ•´æœ¬å°è¯´å¿«é€Ÿå¤„ç†å®Œæˆï¼

ğŸ“– å°è¯´åç§°ï¼šã€Š{self.current_novel_title}ã€‹
ğŸ“Š å¤„ç†ç»Ÿè®¡ï¼šå…±å¤„ç† {total_paragraphs} ä¸ªæ®µè½
ğŸ“„ åˆ†ææŠ¥å‘Šï¼šå·²ä¿å­˜ä¸º {filename}
âš¡ åˆ†ææ¨¡å¼ï¼šç®€åŒ–æ¨¡å¼ï¼ˆå…³é—­æ€ç»´é“¾ä»¥æé«˜é€Ÿåº¦ï¼‰

ğŸ¯ åˆ†æåŒ…å«ï¼š
â€¢ æ¯æ®µè½çš„å…³é”®è¯æ±‡åˆ†æ
â€¢ é‡è¦æ–‡åŒ–èƒŒæ™¯è¯´æ˜
â€¢ åŸºç¡€æ–‡å­¦æŠ€å·§åˆ†æ
â€¢ ç²¾å‡†çš„ä¸­æ–‡ç¿»è¯‘

ğŸ“š æ‚¨å¯ä»¥æ‰“å¼€ {filename} æŸ¥çœ‹å®Œæ•´çš„åˆ†ææŠ¥å‘Šï¼

ğŸ’¡ æç¤ºï¼šå¦‚éœ€è¯¦ç»†åˆ†æï¼Œè¯·ä½¿ç”¨"å¤„ç†ä¸‹ä¸€æ®µ"åŠŸèƒ½é€æ®µåˆ†æã€‚"""
            
            logger.info(f"æ•´æœ¬å°è¯´å¿«é€Ÿå¤„ç†å®Œæˆï¼ŒæŠ¥å‘Šä¿å­˜ä¸ºï¼š{filename}")
            return final_message
            
        except Exception as e:
            error_message = f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}"
            logger.error(error_message)
            return error_message
    
    def save_enhanced_analysis(self) -> str:
        """ä¿å­˜å¢å¼ºåˆ†æç»“æœ"""
        if not self.reader.processed_paragraphs:
            return "âŒ æ²¡æœ‰å·²å¤„ç†çš„æ®µè½å¯ä»¥ä¿å­˜"
        
        filename = self.reader.create_enhanced_docx(self.current_novel_title)
        return f"âœ… å¢å¼ºåˆ†ææŠ¥å‘Šå·²ä¿å­˜ä¸º {filename}ï¼Œå…±åŒ…å« {len(self.reader.processed_paragraphs)} ä¸ªæ®µè½çš„è¯¦ç»†åˆ†æ"

def create_enhanced_interface():
    """åˆ›å»ºå¢å¼ºç‰ˆGradioç•Œé¢"""
    interface = EnhancedGradioInterface()
    
    with gr.Blocks(title="è‹±æ–‡å°è¯´é˜…è¯»è¾…åŠ©è½¯ä»¶", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ“š è‹±æ–‡å°è¯´é˜…è¯»è¾…åŠ©è½¯ä»¶")
        gr.Markdown("Designed by Toby")
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("## ğŸ¤– æ¨¡å‹è®¾ç½®")
                model_dropdown = gr.Dropdown(
                    choices=interface.reader.available_models,
                    value=interface.current_model,
                    label="é€‰æ‹©AIæ¨¡å‹",
                    info="ä¸åŒæ¨¡å‹æœ‰ä¸åŒçš„ç‰¹ç‚¹ï¼Œå¯æ ¹æ®éœ€è¦åˆ‡æ¢"
                )
                model_status = gr.Textbox(label="æ¨¡å‹çŠ¶æ€", interactive=False)
                
                gr.Markdown("## ğŸ“ æ–‡ä»¶åŠ è½½ä¸éš¾åº¦åˆ†æ")
                
                # æ·»åŠ æ–‡ä»¶ä¸Šä¼ é€‰é¡¹
                with gr.Tabs():
                    with gr.TabItem("ğŸ“ ä¸Šä¼ æ–‡ä»¶"):
                        file_upload = gr.File(
                            label="é€‰æ‹©å°è¯´æ–‡ä»¶ (.txt æˆ– .md)",
                            file_types=['.txt', '.md'],
                            type="filepath"
                        )
                        upload_btn = gr.Button("ğŸ“¤ ä¸Šä¼ å¹¶åˆ†æ", variant="primary")
                    
                    with gr.TabItem("ğŸ“‚ æœ¬åœ°æ–‡ä»¶"):
                        file_input = gr.Textbox(
                            label="å°è¯´æ–‡ä»¶è·¯å¾„",
                            placeholder="è¾“å…¥å°è¯´æ–‡ä»¶çš„å®Œæ•´è·¯å¾„...",
                            value="The English Patient.txt"
                        )
                        load_btn = gr.Button("ğŸ“Š åŠ è½½å¹¶åˆ†æ", variant="primary")
                
                load_status = gr.Textbox(label="åŠ è½½çŠ¶æ€", interactive=False)
                
                difficulty_analysis = gr.Textbox(
                    label="ğŸ“Š éš¾åº¦åˆ†ææŠ¥å‘Š", 
                    lines=12, 
                    interactive=False,
                    placeholder="æ–‡æœ¬éš¾åº¦åˆ†æå°†åœ¨è¿™é‡Œæ˜¾ç¤º..."
                )
                
                gr.Markdown("## ğŸ¯ å¤„ç†é€‰é¡¹")
                gr.Markdown("**ğŸ“ å¤„ç†æ¨¡å¼è¯´æ˜ï¼š**")
                gr.Markdown("â€¢ **å¤„ç†ä¸‹ä¸€æ®µ**ï¼šè¯¦ç»†åˆ†ææ¨¡å¼ï¼ŒåŒ…å«å®Œæ•´æ€ç»´é“¾ï¼Œé€‚åˆæ·±åº¦å­¦ä¹ ")
                gr.Markdown("â€¢ **å¤„ç†æ•´æœ¬å°è¯´**ï¼šå¿«é€Ÿåˆ†ææ¨¡å¼ï¼Œå…³é—­æ€ç»´é“¾ï¼Œå¤§å¹…æé«˜å¤„ç†é€Ÿåº¦")
                
                progress_info = gr.Textbox(label="å¤„ç†è¿›åº¦", interactive=False)
                
                with gr.Row():
                    next_btn = gr.Button("â¡ï¸ å¤„ç†ä¸‹ä¸€æ®µï¼ˆè¯¦ç»†æ¨¡å¼ï¼‰", variant="secondary")
                    process_all_btn = gr.Button("ğŸš€ å¤„ç†æ•´æœ¬å°è¯´ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰", variant="primary")
                
                with gr.Row():
                    save_btn = gr.Button("ğŸ’¾ ä¿å­˜å½“å‰åˆ†æ", variant="secondary")
        
        with gr.Row():
            with gr.Column(scale=2):
                current_difficulty = gr.Textbox(
                    label="ğŸ“Š å½“å‰æ®µè½éš¾åº¦ä¿¡æ¯",
                    lines=6,
                    interactive=False,
                    placeholder="æ®µè½éš¾åº¦ä¿¡æ¯å°†åœ¨è¿™é‡Œæ˜¾ç¤º..."
                )
                
                gr.Markdown("## ğŸ“– è‹±æ–‡åŸæ–‡")
                original_text = gr.Textbox(
                    label="åŸæ–‡æ®µè½",
                    lines=8,
                    interactive=False,
                    placeholder="è‹±æ–‡åŸæ–‡å°†åœ¨è¿™é‡Œæ˜¾ç¤º..."
                )
                
                gr.Markdown("## ğŸ” æ·±åº¦åˆ†æç»“æœ")
                analysis_result = gr.Textbox(
                    label="ä¸“ä¸šåˆ†æ",
                    lines=25,
                    interactive=False,
                    placeholder="è¯¦ç»†çš„ä¸“ä¸šåˆ†æç»“æœå°†åœ¨è¿™é‡Œæ˜¾ç¤º..."
                )
        
        # äº‹ä»¶ç»‘å®š
        model_dropdown.change(
            fn=interface.change_model,
            inputs=[model_dropdown],
            outputs=[model_status]
        )
        
        upload_btn.click(
            fn=interface.handle_file_upload,
            inputs=[file_upload],
            outputs=[load_status, difficulty_analysis]
        )
        
        load_btn.click(
            fn=interface.load_and_analyze_novel,
            inputs=[file_input],
            outputs=[load_status, difficulty_analysis]
        )
        
        next_btn.click(
            fn=interface.process_next_paragraph,
            inputs=[],
            outputs=[progress_info, current_difficulty, original_text, analysis_result]
        )
        
        process_all_btn.click(
            fn=interface.process_entire_novel,
            inputs=[],
            outputs=[progress_info]
        )
        
        save_btn.click(
            fn=interface.save_enhanced_analysis,
            inputs=[],
            outputs=[progress_info]
        )
    
    return demo

if __name__ == "__main__":
    demo = create_enhanced_interface()
    demo.launch(
        server_name="127.0.0.1",
        server_port=7862,
        share=False,
        show_error=True
    ) 